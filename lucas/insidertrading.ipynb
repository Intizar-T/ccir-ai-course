{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Insider Trading Data Preparation\n",
        "\n",
        "**Data Source:** This notebook now uses yfinance API to fetch real-time market data instead of the static text file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load CIK-to-Ticker Mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 8031 CIK-to-ticker mappings\n"
          ]
        }
      ],
      "source": [
        "with open('Company Tickers.json', 'r') as f:\n",
        "    cik_ticker_data = json.load(f)\n",
        "\n",
        "cik_mapping = pd.DataFrame(cik_ticker_data).T\n",
        "cik_mapping['cik_str'] = cik_mapping['cik_str'].astype(int)\n",
        "cik_to_ticker = dict(zip(cik_mapping['cik_str'], cik_mapping['ticker']))\n",
        "\n",
        "print(f\"Loaded {len(cik_to_ticker)} CIK-to-ticker mappings\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nonderiv_trans: (59678, 28)\n",
            "submission: (36421, 14)\n",
            "reportingowner: (39602, 13)\n"
          ]
        }
      ],
      "source": [
        "nonderiv_trans = pd.read_csv('NONDERIV_TRANS.tsv', sep='\\t', low_memory=False)\n",
        "submission = pd.read_csv('SUBMISSION.tsv', sep='\\t', low_memory=False)\n",
        "reportingowner = pd.read_csv('REPORTINGOWNER.tsv', sep='\\t', low_memory=False)\n",
        "\n",
        "print(f\"nonderiv_trans: {nonderiv_trans.shape}\")\n",
        "print(f\"submission: {submission.shape}\")\n",
        "print(f\"reportingowner: {reportingowner.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Trim and Merge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Merged: (70722, 12)\n"
          ]
        }
      ],
      "source": [
        "nonderiv_trans = nonderiv_trans[['ACCESSION_NUMBER', 'TRANS_DATE', 'TRANS_CODE',\n",
        "                                  'TRANS_SHARES', 'TRANS_PRICEPERSHARE',\n",
        "                                  'DIRECT_INDIRECT_OWNERSHIP']].copy()\n",
        "\n",
        "submission = submission[['ACCESSION_NUMBER', 'ISSUERTRADINGSYMBOL', 'ISSUERCIK']].copy()\n",
        "\n",
        "reportingowner['IS_DIRECTOR'] = reportingowner['RPTOWNER_RELATIONSHIP'].str.contains('Director', case=False, na=False).astype(int)\n",
        "reportingowner['IS_OFFICER'] = reportingowner['RPTOWNER_RELATIONSHIP'].str.contains('Officer', case=False, na=False).astype(int)\n",
        "reportingowner['IS_TEN_PERCENT_OWNER'] = reportingowner['RPTOWNER_RELATIONSHIP'].str.contains('10%', case=False, na=False).astype(int)\n",
        "reportingowner = reportingowner[['ACCESSION_NUMBER', 'IS_DIRECTOR', 'IS_OFFICER',\n",
        "                                  'IS_TEN_PERCENT_OWNER', 'RPTOWNER_TITLE']].copy()\n",
        "\n",
        "df = nonderiv_trans.merge(submission, on='ACCESSION_NUMBER', how='inner')\n",
        "df = df.merge(reportingowner, on='ACCESSION_NUMBER', how='inner')\n",
        "\n",
        "print(f\"Merged: {df.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Standardize Tickers Using CIK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CIK-mapped tickers: 68577\n",
            "Unmapped (using original): 2145\n"
          ]
        }
      ],
      "source": [
        "df['ISSUERCIK'] = df['ISSUERCIK'].astype(int)\n",
        "df['ticker_from_cik'] = df['ISSUERCIK'].map(cik_to_ticker)\n",
        "df['ticker_standardized'] = df['ticker_from_cik'].fillna(df['ISSUERTRADINGSYMBOL'])\n",
        "\n",
        "print(f\"CIK-mapped tickers: {df['ticker_from_cik'].notna().sum()}\")\n",
        "print(f\"Unmapped (using original): {df['ticker_from_cik'].isna().sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Filter to Open Market Purchases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After filtering: (36750, 14)\n"
          ]
        }
      ],
      "source": [
        "df = df[df['TRANS_CODE'].isin(['P', 'S'])].copy()\n",
        "df = df[df['TRANS_SHARES'].notna() & (df['TRANS_SHARES'] > 0)]\n",
        "df = df[df['TRANS_PRICEPERSHARE'].notna() & (df['TRANS_PRICEPERSHARE'] > 0)]\n",
        "df = df[df['TRANS_DATE'].notna() & df['ticker_standardized'].notna()]\n",
        "\n",
        "print(f\"After filtering: {df.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Transaction-Level Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Clean transactions: (27561, 13)\n",
            "  ticker      cik transaction_date  is_buy   shares  price_per_share  \\\n",
            "0      A  1090872       2025-11-12       0  12490.0           150.00   \n",
            "1      A  1090872       2025-11-18       0    911.0           143.24   \n",
            "2      A  1090872       2025-11-26       0   3000.0           154.99   \n",
            "3      A  1090872       2025-12-01       0   2600.0           149.81   \n",
            "4    AAL     6201       2025-12-09       0  62507.0            15.01   \n",
            "\n",
            "   transaction_value  is_officer  is_director  is_ten_pct_owner  \\\n",
            "0         1873500.00           1            1                 0   \n",
            "1          130491.64           1            1                 0   \n",
            "2          464970.00           1            0                 0   \n",
            "3          389506.00           0            1                 0   \n",
            "4          938230.07           1            0                 0   \n",
            "\n",
            "                 officer_title ownership_type      accession_number  \n",
            "0            President and CEO              D  0001090872-25-000039  \n",
            "1            President and CEO              D  0001090872-25-000063  \n",
            "2   V.P., Corporate Controller              D  0001090872-25-000077  \n",
            "3                          NaN              D  0001090872-25-000083  \n",
            "4  EVP Chief Operating Officer              D  0001934073-25-000005  \n"
          ]
        }
      ],
      "source": [
        "df['TRANS_DATE'] = pd.to_datetime(df['TRANS_DATE'])\n",
        "df['IS_BUY'] = (df['TRANS_CODE'] == 'P').astype(int)\n",
        "df['TRANSACTION_VALUE'] = df['TRANS_SHARES'] * df['TRANS_PRICEPERSHARE']\n",
        "\n",
        "transactions = df[['ticker_standardized', 'ISSUERCIK', 'TRANS_DATE', 'IS_BUY',\n",
        "                   'TRANS_SHARES', 'TRANS_PRICEPERSHARE', 'TRANSACTION_VALUE',\n",
        "                   'IS_OFFICER', 'IS_DIRECTOR', 'IS_TEN_PERCENT_OWNER', 'RPTOWNER_TITLE',\n",
        "                   'DIRECT_INDIRECT_OWNERSHIP', 'ACCESSION_NUMBER']].copy()\n",
        "\n",
        "transactions.columns = ['ticker', 'cik', 'transaction_date', 'is_buy', 'shares',\n",
        "                       'price_per_share', 'transaction_value', 'is_officer', 'is_director',\n",
        "                       'is_ten_pct_owner', 'officer_title', 'ownership_type', 'accession_number']\n",
        "\n",
        "transactions = transactions.drop_duplicates().sort_values(['ticker', 'transaction_date']).reset_index(drop=True)\n",
        "\n",
        "print(f\"Clean transactions: {transactions.shape}\")\n",
        "print(transactions.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Event-Level Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ML-ready features: (1442, 9)\n",
            "  ticker transaction_date  num_buy_transactions  total_shares_bought  \\\n",
            "0   AAON       2025-12-12                     1              6141.00   \n",
            "1   AARD       2025-12-11                     2             10000.00   \n",
            "2  ABAKF       2025-10-15                     1               534.00   \n",
            "3   ABCB       2025-12-08                     1               307.58   \n",
            "4   ABCL       2025-11-26                     1             50000.00   \n",
            "\n",
            "   total_value_bought  has_officer_buy  has_director_buy  \\\n",
            "0         484400.1900                1                 0   \n",
            "1         144630.0000                1                 1   \n",
            "2           8314.3800                0                 1   \n",
            "3          23751.3276                0                 1   \n",
            "4         178500.0000                0                 1   \n",
            "\n",
            "   has_ten_pct_owner_buy  avg_transaction_value  \n",
            "0                      0            484400.1900  \n",
            "1                      0             72315.0000  \n",
            "2                      0              8314.3800  \n",
            "3                      0             23751.3276  \n",
            "4                      0            178500.0000  \n"
          ]
        }
      ],
      "source": [
        "buys = transactions[transactions['is_buy'] == 1].copy()\n",
        "buys['window_id'] = (buys['transaction_date'] - buys['transaction_date'].min()).dt.days // 30\n",
        "\n",
        "features = buys.groupby(['ticker', 'window_id']).agg({\n",
        "    'transaction_date': 'max',  # Use most recent transaction date in window\n",
        "    'accession_number': 'nunique',\n",
        "    'shares': 'sum',\n",
        "    'transaction_value': 'sum',\n",
        "    'is_officer': 'max',\n",
        "    'is_director': 'max',\n",
        "    'is_ten_pct_owner': 'max'\n",
        "}).reset_index()\n",
        "\n",
        "features.columns = ['ticker', 'window_id', 'transaction_date',\n",
        "                   'num_buy_transactions', 'total_shares_bought', 'total_value_bought',\n",
        "                   'has_officer_buy', 'has_director_buy', 'has_ten_pct_owner_buy']\n",
        "\n",
        "features['avg_transaction_value'] = features['total_value_bought'] / features['num_buy_transactions']\n",
        "\n",
        "# Drop window_id as we'll use transaction_date for merging\n",
        "features = features.drop('window_id', axis=1)\n",
        "\n",
        "print(f\"ML-ready features: {features.shape}\")\n",
        "print(features.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fetch Market Data from yfinance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of unique tickers to fetch: 2621\n",
            "Transaction date range: 2005-11-20 to 2025-12-31\n",
            "Market data fetch range: 2005-08-22 to 2026-03-01\n",
            "  (90 days before + 60 days after transactions for forward returns)\n",
            "\n",
            "Fetching market data... This may take a few minutes.\n",
            "\n",
            "Processing batch 1/27 (100 tickers)... ✓ 100 successful\n",
            "Processing batch 2/27 (100 tickers)... ✓ 100 successful\n",
            "Processing batch 3/27 (100 tickers)... ✓ 100 successful\n",
            "Processing batch 4/27 (100 tickers)... ✓ 100 successful\n",
            "Processing batch 5/27 (100 tickers)... ✓ 100 successful\n",
            "Processing batch 6/27 (100 tickers)... ✓ 100 successful\n",
            "Processing batch 7/27 (100 tickers)... ✓ 100 successful\n",
            "Processing batch 8/27 (100 tickers)..."
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "$EPDU: possibly delisted; no timezone found\n",
            "\n",
            "1 Failed download:\n",
            "['EPDU']: possibly delisted; no timezone found\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ✓ 99 successful\n",
            "Processing batch 9/27 (100 tickers)... ✓ 100 successful\n",
            "Processing batch 10/27 (100 tickers)..."
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "$FOACW: possibly delisted; no timezone found\n",
            "\n",
            "1 Failed download:\n",
            "['FOACW']: possibly delisted; no timezone found\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ✓ 99 successful\n",
            "Processing batch 11/27 (100 tickers)..."
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "$GORV: possibly delisted; no price data found  (1d 2005-08-22 00:00:00 -> 2026-03-01 00:00:00) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "\n",
            "1 Failed download:\n",
            "['GORV']: possibly delisted; no price data found  (1d 2005-08-22 00:00:00 -> 2026-03-01 00:00:00) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "HTTP Error 404: {\"quoteSummary\":{\"result\":null,\"error\":{\"code\":\"Not Found\",\"description\":\"Quote not found for symbol: GORV\"}}}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ✓ 99 successful\n",
            "Processing batch 12/27 (100 tickers)..."
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "$HIPOW: possibly delisted; no timezone found\n",
            "\n",
            "1 Failed download:\n",
            "['HIPOW']: possibly delisted; no timezone found\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ✓ 99 successful\n",
            "Processing batch 13/27 (100 tickers)..."
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "$INFA: possibly delisted; no timezone found\n",
            "\n",
            "1 Failed download:\n",
            "['INFA']: possibly delisted; no timezone found\n",
            "HTTP Error 404: {\"quoteSummary\":{\"result\":null,\"error\":{\"code\":\"Not Found\",\"description\":\"Quote not found for symbol: INFA\"}}}\n",
            "$IVNHW: possibly delisted; no timezone found\n",
            "\n",
            "1 Failed download:\n",
            "['IVNHW']: possibly delisted; no timezone found\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ✓ 98 successful\n",
            "Processing batch 14/27 (100 tickers)..."
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "$LAWIL: possibly delisted; no timezone found\n",
            "\n",
            "1 Failed download:\n",
            "['LAWIL']: possibly delisted; no timezone found\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ✓ 99 successful\n",
            "Processing batch 15/27 (100 tickers)..."
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "$LVTX: possibly delisted; no timezone found\n",
            "\n",
            "1 Failed download:\n",
            "['LVTX']: possibly delisted; no timezone found\n",
            "HTTP Error 404: {\"quoteSummary\":{\"result\":null,\"error\":{\"code\":\"Not Found\",\"description\":\"Quote not found for symbol: LVTX\"}}}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ✓ 99 successful\n",
            "Processing batch 16/27 (100 tickers)..."
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "$MURA: possibly delisted; no price data found  (1d 2005-08-22 00:00:00 -> 2026-03-01 00:00:00) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "\n",
            "1 Failed download:\n",
            "['MURA']: possibly delisted; no price data found  (1d 2005-08-22 00:00:00 -> 2026-03-01 00:00:00) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "HTTP Error 404: {\"quoteSummary\":{\"result\":null,\"error\":{\"code\":\"Not Found\",\"description\":\"Quote not found for symbol: MURA\"}}}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ✓ 99 successful\n",
            "Processing batch 17/27 (100 tickers)..."
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "$NEUE: possibly delisted; no timezone found\n",
            "\n",
            "1 Failed download:\n",
            "['NEUE']: possibly delisted; no timezone found\n",
            "HTTP Error 404: {\"quoteSummary\":{\"result\":null,\"error\":{\"code\":\"Not Found\",\"description\":\"Quote not found for symbol: NEUE\"}}}\n",
            "$NONE: possibly delisted; no timezone found\n",
            "\n",
            "1 Failed download:\n",
            "['NONE']: possibly delisted; no timezone found\n",
            "HTTP Error 404: {\"quoteSummary\":{\"result\":null,\"error\":{\"code\":\"Not Found\",\"description\":\"Quote not found for symbol: NONE\"}}}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ✓ 98 successful\n",
            "Processing batch 18/27 (100 tickers)... ✓ 100 successful\n",
            "Processing batch 19/27 (100 tickers)..."
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "$PGIM: possibly delisted; no timezone found\n",
            "\n",
            "1 Failed download:\n",
            "['PGIM']: possibly delisted; no timezone found\n",
            "HTTP Error 404: {\"quoteSummary\":{\"result\":null,\"error\":{\"code\":\"Not Found\",\"description\":\"Quote not found for symbol: PGIM\"}}}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ✓ 99 successful\n",
            "Processing batch 20/27 (100 tickers)..."
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "$PRTFTM: possibly delisted; no timezone found\n",
            "\n",
            "1 Failed download:\n",
            "['PRTFTM']: possibly delisted; no timezone found\n",
            "HTTP Error 404: {\"quoteSummary\":{\"result\":null,\"error\":{\"code\":\"Not Found\",\"description\":\"Quote not found for symbol: PRTFTM\"}}}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ✓ 99 successful\n",
            "Processing batch 21/27 (100 tickers)..."
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "$RRAC: possibly delisted; no timezone found\n",
            "\n",
            "1 Failed download:\n",
            "['RRAC']: possibly delisted; no timezone found\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ✓ 99 successful\n",
            "Processing batch 22/27 (100 tickers)... ✓ 100 successful\n",
            "Processing batch 23/27 (100 tickers)..."
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "$SSST: possibly delisted; no price data found  (1d 2005-08-22 00:00:00 -> 2026-03-01 00:00:00) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "\n",
            "1 Failed download:\n",
            "['SSST']: possibly delisted; no price data found  (1d 2005-08-22 00:00:00 -> 2026-03-01 00:00:00) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "HTTP Error 404: {\"quoteSummary\":{\"result\":null,\"error\":{\"code\":\"Not Found\",\"description\":\"Quote not found for symbol: SSST\"}}}\n",
            "$STGC: possibly delisted; no timezone found\n",
            "\n",
            "1 Failed download:\n",
            "['STGC']: possibly delisted; no timezone found\n",
            "HTTP Error 404: {\"quoteSummary\":{\"result\":null,\"error\":{\"code\":\"Not Found\",\"description\":\"Quote not found for symbol: STGC\"}}}\n",
            "$TEAF: possibly delisted; no timezone found\n",
            "\n",
            "1 Failed download:\n",
            "['TEAF']: possibly delisted; no timezone found\n",
            "HTTP Error 404: {\"quoteSummary\":{\"result\":null,\"error\":{\"code\":\"Not Found\",\"description\":\"Quote not found for symbol: TEAF\"}}}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ✓ 97 successful\n",
            "Processing batch 24/27 (100 tickers)..."
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "$TICAW: possibly delisted; no timezone found\n",
            "\n",
            "1 Failed download:\n",
            "['TICAW']: possibly delisted; no timezone found\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ✓ 99 successful\n",
            "Processing batch 25/27 (100 tickers)..."
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "$VBTX: possibly delisted; no timezone found\n",
            "\n",
            "1 Failed download:\n",
            "['VBTX']: possibly delisted; no timezone found\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ✓ 99 successful\n",
            "Processing batch 26/27 (100 tickers)..."
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "$XHFIX: possibly delisted; no timezone found\n",
            "\n",
            "1 Failed download:\n",
            "['XHFIX']: possibly delisted; no timezone found\n",
            "$XPEPX: possibly delisted; no price data found  (1d 2005-08-22 00:00:00 -> 2026-03-01 00:00:00)\n",
            "\n",
            "1 Failed download:\n",
            "['XPEPX']: possibly delisted; no price data found  (1d 2005-08-22 00:00:00 -> 2026-03-01 00:00:00)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ✓ 98 successful\n",
            "Processing batch 27/27 (21 tickers)..."
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "$[NONE]: possibly delisted; no timezone found\n",
            "\n",
            "1 Failed download:\n",
            "['[NONE]']: possibly delisted; no timezone found\n",
            "HTTP Error 404: {\"quoteSummary\":{\"result\":null,\"error\":{\"code\":\"Not Found\",\"description\":\"Quote not found for symbol: [NONE]\"}}}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ✓ 20 successful\n",
            "\n",
            "============================================================\n",
            "MARKET DATA FETCH COMPLETE\n",
            "============================================================\n",
            "Successfully fetched: 2599 tickers (99.2%)\n",
            "Failed/Delisted: 22 tickers (0.8%)\n",
            "Total price records: 8,130,022\n",
            "Date range: 2005-08-22 to 2026-02-06\n",
            "Shares outstanding fetched for: 2292 tickers\n",
            "\n",
            "Sample failed tickers (likely delisted/warrants): ['EPDU', 'FOACW', 'GORV', 'HIPOW', 'INFA', 'IVNHW', 'LAWIL', 'LVTX', 'MURA', 'NEUE']\n"
          ]
        }
      ],
      "source": [
        "# Install yfinance if not already installed\n",
        "try:\n",
        "    import yfinance as yf\n",
        "except ImportError:\n",
        "    print(\"Installing yfinance...\")\n",
        "    import subprocess\n",
        "    subprocess.check_call(['pip', 'install', 'yfinance'])\n",
        "    import yfinance as yf\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')  # Suppress yfinance warnings\n",
        "\n",
        "# Get unique tickers from insider trading data\n",
        "unique_tickers = transactions['ticker'].unique()\n",
        "print(f\"Number of unique tickers to fetch: {len(unique_tickers)}\")\n",
        "\n",
        "# Determine date range based on transaction dates - ALIGNED TO Q4 2025\n",
        "min_transaction_date = transactions['transaction_date'].min()\n",
        "max_transaction_date = transactions['transaction_date'].max()\n",
        "\n",
        "# Get 90 days before earliest transaction (for context) and 60 days after latest (for forward returns)\n",
        "start_date = min_transaction_date - pd.Timedelta(days=90)\n",
        "end_date = max_transaction_date + pd.Timedelta(days=60)\n",
        "\n",
        "print(f\"Transaction date range: {min_transaction_date.date()} to {max_transaction_date.date()}\")\n",
        "print(f\"Market data fetch range: {start_date.date()} to {end_date.date()}\")\n",
        "print(f\"  (90 days before + 60 days after transactions for forward returns)\")\n",
        "print(f\"\\nFetching market data... This may take a few minutes.\\n\")\n",
        "\n",
        "# Fetch data for all tickers\n",
        "prices_list = []\n",
        "shares_outstanding_dict = {}  # Store shares outstanding data\n",
        "failed_tickers = []\n",
        "success_count = 0\n",
        "batch_size = 100\n",
        "\n",
        "for i in range(0, len(unique_tickers), batch_size):\n",
        "    batch = unique_tickers[i:i+batch_size]\n",
        "    batch_num = i//batch_size + 1\n",
        "    total_batches = (len(unique_tickers)-1)//batch_size + 1\n",
        "    \n",
        "    print(f\"Processing batch {batch_num}/{total_batches} ({len(batch)} tickers)...\", end='')\n",
        "    \n",
        "    batch_success = 0\n",
        "    for ticker in batch:\n",
        "        try:\n",
        "            # Download data for this ticker (suppress output)\n",
        "            import sys\n",
        "            from io import StringIO\n",
        "            old_stdout = sys.stdout\n",
        "            sys.stdout = StringIO()\n",
        "            \n",
        "            ticker_data = yf.download(ticker, start=start_date, end=end_date, progress=False, auto_adjust=True)\n",
        "            \n",
        "            # Get shares outstanding from ticker info\n",
        "            ticker_obj = yf.Ticker(ticker)\n",
        "            try:\n",
        "                shares_outstanding = ticker_obj.info.get('sharesOutstanding', None)\n",
        "                if shares_outstanding:\n",
        "                    shares_outstanding_dict[ticker] = shares_outstanding\n",
        "            except:\n",
        "                pass\n",
        "            \n",
        "            sys.stdout = old_stdout\n",
        "            \n",
        "            if not ticker_data.empty:\n",
        "                # Reset index to get date as a column\n",
        "                ticker_data = ticker_data.reset_index()\n",
        "                ticker_data['ticker'] = ticker\n",
        "                \n",
        "                # Rename columns to match expected format\n",
        "                ticker_data.columns = ['date', 'open', 'high', 'low', 'close', 'volume', 'ticker']\n",
        "                \n",
        "                prices_list.append(ticker_data[['ticker', 'date', 'open', 'high', 'low', 'close', 'volume']])\n",
        "                batch_success += 1\n",
        "                success_count += 1\n",
        "            else:\n",
        "                failed_tickers.append(ticker)\n",
        "        except Exception as e:\n",
        "            sys.stdout = old_stdout\n",
        "            failed_tickers.append(ticker)\n",
        "            continue\n",
        "    \n",
        "    print(f\" ✓ {batch_success} successful\")\n",
        "\n",
        "# Combine all data\n",
        "if prices_list:\n",
        "    prices = pd.concat(prices_list, ignore_index=True)\n",
        "    \n",
        "    # Clean and preprocess\n",
        "    prices['date'] = pd.to_datetime(prices['date'])\n",
        "    prices = prices[prices['close'].notna() & (prices['close'] > 0)]\n",
        "    prices = prices.sort_values(['ticker', 'date']).reset_index(drop=True)\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"MARKET DATA FETCH COMPLETE\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Successfully fetched: {len(prices['ticker'].unique())} tickers ({success_count/len(unique_tickers)*100:.1f}%)\")\n",
        "    print(f\"Failed/Delisted: {len(failed_tickers)} tickers ({len(failed_tickers)/len(unique_tickers)*100:.1f}%)\")\n",
        "    print(f\"Total price records: {prices.shape[0]:,}\")\n",
        "    print(f\"Date range: {prices['date'].min().date()} to {prices['date'].max().date()}\")\n",
        "    print(f\"Shares outstanding fetched for: {len(shares_outstanding_dict)} tickers\")\n",
        "    \n",
        "    # Show sample of failed tickers\n",
        "    if len(failed_tickers) > 0:\n",
        "        print(f\"\\nSample failed tickers (likely delisted/warrants): {failed_tickers[:10]}\")\n",
        "else:\n",
        "    print(\"ERROR: Failed to fetch data for any ticker\")\n",
        "    prices = pd.DataFrame(columns=['ticker', 'date', 'open', 'high', 'low', 'close', 'volume'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validate Market Data Coverage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Insider tickers: 2621\n",
            "Market tickers: 2599\n",
            "Overlapping: 2599\n",
            "Match rate: 99.16%\n",
            "\n",
            "Sample of unmatched tickers: ['[none]', 'HIPOW', 'FOACW', 'XHFIX', 'EPDU', 'TEAF', 'RRAC', 'NEUE', 'VBTX', 'IVNHW', 'LVTX', 'INFA', 'TICAW', 'MURA', 'PRTFTM', 'STGC', 'NONE', 'PGIM', 'SSST', 'GORV']\n",
            "\n",
            "Filtered transactions: (27245, 13)\n",
            "Filtered features: (1432, 9)\n"
          ]
        }
      ],
      "source": [
        "# Check ticker overlap\n",
        "insider_tickers = set(transactions['ticker'].unique())\n",
        "market_tickers = set(prices['ticker'].unique())\n",
        "\n",
        "print(f\"Insider tickers: {len(insider_tickers)}\")\n",
        "print(f\"Market tickers: {len(market_tickers)}\")\n",
        "print(f\"Overlapping: {len(insider_tickers & market_tickers)}\")\n",
        "print(f\"Match rate: {len(insider_tickers & market_tickers) / len(insider_tickers):.2%}\")\n",
        "\n",
        "# Show some examples of unmatched tickers\n",
        "unmatched = insider_tickers - market_tickers\n",
        "if unmatched:\n",
        "    print(f\"\\nSample of unmatched tickers: {list(unmatched)[:20]}\")\n",
        "    \n",
        "# Filter transactions to only include tickers with market data\n",
        "transactions = transactions[transactions['ticker'].isin(market_tickers)].reset_index(drop=True)\n",
        "features = features[features['ticker'].isin(market_tickers)].reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nFiltered transactions: {transactions.shape}\")\n",
        "print(f\"Filtered features: {features.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calculate Forward Returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added 7-day forward returns\n",
            "Rows with complete forward returns: 8112521\n",
            "\n",
            "Return statistics:\n",
            "7-day: mean=6.97%, std=4754.18%\n"
          ]
        }
      ],
      "source": [
        "# Calculate forward returns for 7-day horizon only\n",
        "prices['close_7d'] = prices.groupby('ticker')['close'].shift(-7)\n",
        "prices['return_7d'] = (prices['close_7d'] - prices['close']) / prices['close']\n",
        "\n",
        "# Remove rows where we can't calculate 7-day forward returns (near the end of data)\n",
        "prices_complete = prices[prices['return_7d'].notna()].copy()\n",
        "\n",
        "print(f\"Added 7-day forward returns\")\n",
        "print(f\"Rows with complete forward returns: {prices_complete.shape[0]}\")\n",
        "print(f\"\\nReturn statistics:\")\n",
        "mean_return = prices_complete['return_7d'].mean()\n",
        "std_return = prices_complete['return_7d'].std()\n",
        "print(f\"7-day: mean={mean_return:.2%}, std={std_return:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Merge Transactions with Market Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transactions with prices: (26126, 15)\n",
            "Match rate: 95.89%\n",
            "\n",
            "Sample of transactions with prices:\n",
            "  ticker transaction_date  is_buy   shares  transaction_value       close  \\\n",
            "0      A       2025-11-12       0  12490.0         1873500.00  149.642551   \n",
            "1      A       2025-11-18       0    911.0          130491.64  142.984454   \n",
            "2      A       2025-11-26       0   3000.0          464970.00  155.462155   \n",
            "3      A       2025-12-01       0   2600.0          389506.00  152.267848   \n",
            "4    AAL       2025-12-09       0  62507.0          938230.07   14.730000   \n",
            "5    AAL       2025-12-16       0  25595.0          409775.95   15.360000   \n",
            "6   AAOI       2025-12-10       0   1000.0           35000.00   29.555000   \n",
            "7   AAOI       2025-12-10       0   3000.0           91200.00   29.555000   \n",
            "8   AAOI       2025-12-15       0   4121.0          124577.83   33.680000   \n",
            "9   AAOI       2025-12-23       0  12500.0          501875.00   37.639999   \n",
            "\n",
            "   return_7d  \n",
            "0  -0.036288  \n",
            "1   0.075887  \n",
            "2  -0.070117  \n",
            "3  -0.072309  \n",
            "4   0.063136  \n",
            "5   0.016276  \n",
            "6   0.006767  \n",
            "7   0.006767  \n",
            "8   0.209323  \n",
            "9   0.090728  \n"
          ]
        }
      ],
      "source": [
        "# Merge transactions with price data\n",
        "transactions_with_prices = transactions.merge(\n",
        "    prices[['ticker', 'date', 'close', 'return_7d']],\n",
        "    left_on=['ticker', 'transaction_date'],\n",
        "    right_on=['ticker', 'date'],\n",
        "    how='inner'\n",
        ").drop('date', axis=1)\n",
        "\n",
        "print(f\"Transactions with prices: {transactions_with_prices.shape}\")\n",
        "print(f\"Match rate: {transactions_with_prices.shape[0] / transactions.shape[0]:.2%}\")\n",
        "\n",
        "# Show sample\n",
        "print(f\"\\nSample of transactions with prices:\")\n",
        "print(transactions_with_prices[['ticker', 'transaction_date', 'is_buy', 'shares', \n",
        "                                 'transaction_value', 'close', 'return_7d']].head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Duplicate cell - removed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features with prices: (1335, 13)\n",
            "Match rate: 93.23%\n",
            "Tickers with shares outstanding data: 1202\n",
            "\n",
            "Sample of features with prices:\n",
            "   ticker transaction_date  num_buy_transactions  total_value_bought  \\\n",
            "0    AAON       2025-12-12                     1        4.844002e+05   \n",
            "1    AARD       2025-12-11                     2        1.446300e+05   \n",
            "2   ABAKF       2025-10-15                     1        8.314380e+03   \n",
            "3    ABCB       2025-12-08                     1        2.375133e+04   \n",
            "4    ABCL       2025-11-26                     1        1.785000e+05   \n",
            "5  ABR-PF       2025-11-28                     4        6.470324e+05   \n",
            "6    ABSI       2025-12-05                     1        1.488000e+05   \n",
            "7    ABTC       2025-12-22                     2        4.904800e+05   \n",
            "8    ACET       2025-10-08                     1        5.000000e+06   \n",
            "9    ACNB       2025-11-20                     1        1.584000e+04   \n",
            "\n",
            "   total_shares_bought  total_shares_count  shares_traded_pct      close  \\\n",
            "0              6141.00          81634058.0           0.007523  84.110001   \n",
            "1             10000.00          21773272.0           0.045928  14.400000   \n",
            "2               534.00          33295277.0           0.001604   2.110000   \n",
            "3               307.58          68022316.0           0.000452  76.486147   \n",
            "4             50000.00         299335048.0           0.016704   3.640000   \n",
            "5             74308.00                 NaN                NaN  21.960768   \n",
            "6             40000.00         150371531.0           0.026601   3.730000   \n",
            "7            276000.00         195380091.0           0.141263   1.990000   \n",
            "8           5000000.00           9600000.0          52.083333  13.760000   \n",
            "9               396.00          10372251.0           0.003818  46.452406   \n",
            "\n",
            "   return_7d  \n",
            "0  -0.110451  \n",
            "1  -0.040972  \n",
            "2   0.000000  \n",
            "3   0.017082  \n",
            "4   0.005495  \n",
            "5  -0.004474  \n",
            "6  -0.096515  \n",
            "7  -0.120603  \n",
            "8   0.232558  \n",
            "9   0.036975  \n"
          ]
        }
      ],
      "source": [
        "# Merge features with price data\n",
        "features_with_prices = features.merge(\n",
        "    prices[['ticker', 'date', 'close', 'return_7d']],\n",
        "    left_on=['ticker', 'transaction_date'],\n",
        "    right_on=['ticker', 'date'],\n",
        "    how='inner'\n",
        ").drop('date', axis=1)\n",
        "\n",
        "# Keep only rows with complete forward returns\n",
        "features_with_prices = features_with_prices[features_with_prices['return_7d'].notna()]\n",
        "\n",
        "# Add total shares outstanding\n",
        "features_with_prices['total_shares_count'] = features_with_prices['ticker'].map(shares_outstanding_dict)\n",
        "\n",
        "# Calculate proportion of shares traded vs total shares\n",
        "features_with_prices['shares_traded_pct'] = (\n",
        "    features_with_prices['total_shares_bought'] / features_with_prices['total_shares_count']\n",
        ") * 100\n",
        "\n",
        "print(f\"Features with prices: {features_with_prices.shape}\")\n",
        "print(f\"Match rate: {features_with_prices.shape[0] / features.shape[0]:.2%}\")\n",
        "print(f\"Tickers with shares outstanding data: {features_with_prices['total_shares_count'].notna().sum()}\")\n",
        "\n",
        "# Show sample\n",
        "print(f\"\\nSample of features with prices:\")\n",
        "if len(features_with_prices) > 0:\n",
        "    print(features_with_prices[['ticker', 'transaction_date', 'num_buy_transactions', \n",
        "                                 'total_value_bought', 'total_shares_bought', 'total_shares_count',\n",
        "                                 'shares_traded_pct', 'close', 'return_7d']].head(10))\n",
        "else:\n",
        "    print(\"No matches found - please run the yfinance data fetching cell above\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Binary Labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Duplicate header - removed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Labels created with 5.0% threshold\n",
            "\n",
            "Label distribution (7-day):\n",
            "label_7d\n",
            "0    977\n",
            "1    358\n",
            "Name: count, dtype: int64\n",
            "Positive rate: 26.82%\n"
          ]
        }
      ],
      "source": [
        "threshold = 0.05\n",
        "\n",
        "if len(features_with_prices) > 0:\n",
        "    features_with_prices['label_7d'] = (features_with_prices['return_7d'] > threshold).astype(int)\n",
        "\n",
        "    print(f\"Labels created with {threshold:.1%} threshold\")\n",
        "    print(f\"\\nLabel distribution (7-day):\")\n",
        "    print(features_with_prices['label_7d'].value_counts())\n",
        "    print(f\"Positive rate: {features_with_prices['label_7d'].mean():.2%}\")\n",
        "else:\n",
        "    print(\"No features with prices - cannot create labels. Please run yfinance data fetching cell above.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Final Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "DATA PREPARATION COMPLETE\n",
            "================================================================================\n",
            "\n",
            "Saved files:\n",
            "  transactions_clean.csv: 27245 rows\n",
            "  transactions_with_prices.csv: 26126 rows\n",
            "  features_ml_ready.csv: 1432 rows\n",
            "  features_with_labels.csv: 1335 rows (READY FOR ML)\n",
            "  market_prices_clean.csv: 8130022 rows\n"
          ]
        }
      ],
      "source": [
        "transactions.to_csv('transactions_clean.csv', index=False)\n",
        "transactions_with_prices.to_csv('transactions_with_prices.csv', index=False)\n",
        "features.to_csv('features_ml_ready.csv', index=False)\n",
        "features_with_prices.to_csv('features_with_labels.csv', index=False)\n",
        "prices.to_csv('market_prices_clean.csv', index=False)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"DATA PREPARATION COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nSaved files:\")\n",
        "print(f\"  transactions_clean.csv: {transactions.shape[0]} rows\")\n",
        "print(f\"  transactions_with_prices.csv: {transactions_with_prices.shape[0]} rows\")\n",
        "print(f\"  features_ml_ready.csv: {features.shape[0]} rows\")\n",
        "print(f\"  features_with_labels.csv: {features_with_prices.shape[0]} rows (READY FOR ML)\")\n",
        "print(f\"  market_prices_clean.csv: {prices.shape[0]} rows\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
