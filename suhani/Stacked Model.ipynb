{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "983961b6-65a2-4f1a-84fe-b4e29687cc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: load → preprocess → feature_eng\n",
      "  Rows: 700 → 700 (impute=True) → 686\n",
      "  Features: 30 cols\n",
      "  Split: random\n",
      "  Train / Val / Test: 548 / 68 / 70\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/research2/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model comparison (Validation | Test)\n",
      "------------------------------------------------------------------------\n",
      "Model               Val MAE Val RMSE   Val R²  |  Test MAE Test RMSE  Test R²\n",
      "------------------------------------------------------------------------\n",
      "Dummy (mean)         6.2069   7.7226  -0.0328  |    5.4123   6.7008  -0.0000\n",
      "Ridge (CV)           4.6838   6.0168   0.3730  |    4.4826   5.5009   0.3261\n",
      "Ridge (log y)        4.7407   5.9091   0.3953  |    4.5205   5.5500   0.3140\n",
      "ElasticNet (CV)      4.6437   5.9752   0.3817  |    4.5115   5.5227   0.3207\n",
      "Ridge (reduced)      4.6802   6.0083   0.3748  |    4.4797   5.4932   0.3279\n",
      "Random Forest        4.6471   5.7835   0.4207  |    4.2895   5.4263   0.3442\n",
      "Gradient Boosting    4.7448   5.7326   0.4309  |    4.2758   5.3849   0.3542\n",
      "Hist Gradient Boosting   4.7035   5.8802   0.4012  |    4.5056   5.6763   0.2824\n",
      "Extra Trees          4.5217   5.7577   0.4259  |    4.1905   5.1982   0.3982\n",
      "FNN (MLP)            6.9988   8.6286  -0.2894  |    5.4465   6.6940   0.0020\n",
      "SVR                  5.0061   6.4036   0.2898  |    4.6715   5.7596   0.2612\n",
      "XGBoost              4.3905   5.3913   0.4966  |    4.6145   5.7296   0.2689\n",
      "Keras                4.7214   6.0144   0.3735  |    4.6350   5.6537   0.2881\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "Best by Test R²: Extra Trees  (R² = 0.3982)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Predict Number of Admissions from air quality and weather (merged_data.csv).\n",
    "Pipeline: load → preprocess → feature engineering → train/val/test split (time-aware or random, see USE_RANDOM_SPLIT)\n",
    "→ train models → report comparison and best by Test R².\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import ElasticNetCV, RidgeCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.svm import SVR\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "USE_IMPUTATION = True\n",
    "WINSORIZE_QUANTILE = 0.99\n",
    "TRAIN_FRAC, VAL_FRAC, TEST_FRAC = 0.80, 0.10, 0.10\n",
    "USE_RANDOM_SPLIT = True  # True: random split; False: time-aware (chronological) split\n",
    "SPLIT_RANDOM_STATE = 42   # used when USE_RANDOM_SPLIT is True\n",
    "SCALER_TYPE = \"standard\"\n",
    "\n",
    "TIMESTAMP_COL = \"Timestamp\"\n",
    "DATE_COL = \"Date\"\n",
    "TARGET_COL = \"Number of Admissions\"\n",
    "# Engineered features added in feature_engineering() (on top of dataset features).\n",
    "ADMISSION_LAGS_AND_TIME = [\n",
    "    \"admissions_lag1\", \"admissions_lag3\", \"admissions_lag7\", \"admissions_lag14\",\n",
    "    \"dow_sin\", \"dow_cos\", \"month_sin\", \"month_cos\", \"weekend\",\n",
    "]\n",
    "\n",
    "\n",
    "def get_feature_columns(df):\n",
    "    \"\"\"All numeric dataset columns (except timestamp/date and target) + ADMISSION_LAGS_AND_TIME.\"\"\"\n",
    "    exclude = (TARGET_COL, DATE_COL)\n",
    "    dataset_features = [\n",
    "        c for c in df.columns\n",
    "        if c not in exclude and c not in ADMISSION_LAGS_AND_TIME and pd.api.types.is_numeric_dtype(df[c])\n",
    "    ]\n",
    "    engineered = [c for c in ADMISSION_LAGS_AND_TIME if c in df.columns]\n",
    "    return dataset_features + engineered\n",
    "\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    df = pd.read_csv(\"/Users/suhaniagarwal/Downloads/all_features_data.csv\")\n",
    "    if TIMESTAMP_COL in df.columns:\n",
    "        df[DATE_COL] = pd.to_datetime(df[TIMESTAMP_COL], errors=\"coerce\")\n",
    "        df = df.drop(columns=[TIMESTAMP_COL])\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_data(df, impute_features=False):\n",
    "    df = df.sort_values(DATE_COL).copy()\n",
    "    df = df.dropna(subset=[TARGET_COL])\n",
    "    feature_cols = get_feature_columns(df)\n",
    "    if impute_features:\n",
    "        for col in feature_cols:\n",
    "            if col in df.columns and df[col].isna().any():\n",
    "                df[col] = df[col].ffill().bfill()\n",
    "        df = df.dropna(subset=[c for c in feature_cols if c in df.columns])\n",
    "    else:\n",
    "        df = df.dropna(subset=[c for c in feature_cols if c in df.columns])\n",
    "    q = df[TARGET_COL].quantile(WINSORIZE_QUANTILE)\n",
    "    df.loc[df[TARGET_COL] > q, TARGET_COL] = q\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def feature_engineering(df):\n",
    "    \"\"\"Add admission lags and time features only (no rolling means).\"\"\"\n",
    "    df = df.sort_values(DATE_COL).reset_index(drop=True)\n",
    "    dt = pd.to_datetime(df[DATE_COL])\n",
    "    df[\"dow_sin\"] = np.sin(2 * np.pi * dt.dt.dayofweek / 7)\n",
    "    df[\"dow_cos\"] = np.cos(2 * np.pi * dt.dt.dayofweek / 7)\n",
    "    df[\"month_sin\"] = np.sin(2 * np.pi * dt.dt.month / 12)\n",
    "    df[\"month_cos\"] = np.cos(2 * np.pi * dt.dt.month / 12)\n",
    "    df[\"weekend\"] = (dt.dt.dayofweek >= 5).astype(np.float64)\n",
    "    for lag in (1, 3, 7, 14):\n",
    "        df[f\"admissions_lag{lag}\"] = df[TARGET_COL].shift(lag)\n",
    "    adm_cols = [f\"admissions_lag{lag}\" for lag in (1, 3, 7, 14)]\n",
    "    df = df.dropna(how=\"any\", subset=adm_cols)\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def _time_aware_split_indices(n, train_frac, val_frac, test_frac):\n",
    "    \"\"\"Return (train_ix, val_ix, test_ix) as index arrays for chronological split.\"\"\"\n",
    "    n_train = int(n * train_frac)\n",
    "    n_val = int(n * val_frac)\n",
    "    n_test = n - n_train - n_val\n",
    "    train_ix = np.arange(0, n_train)\n",
    "    val_ix = np.arange(n_train, n_train + n_val)\n",
    "    test_ix = np.arange(n_train + n_val, n)\n",
    "    return train_ix, val_ix, test_ix\n",
    "\n",
    "\n",
    "def _random_split_indices(n, train_frac, val_frac, test_frac, random_state=None):\n",
    "    \"\"\"Return (train_ix, val_ix, test_ix) as index arrays for random split.\"\"\"\n",
    "    rng = np.random.default_rng(random_state if random_state is not None else SPLIT_RANDOM_STATE)\n",
    "    perm = rng.permutation(n)\n",
    "    n_train = int(n * train_frac)\n",
    "    n_val = int(n * val_frac)\n",
    "    n_test = n - n_train - n_val\n",
    "    train_ix = perm[:n_train]\n",
    "    val_ix = perm[n_train : n_train + n_val]\n",
    "    test_ix = perm[n_train + n_val :]\n",
    "    return train_ix, val_ix, test_ix\n",
    "\n",
    "\n",
    "def training_data_split(df, train_frac=None, val_frac=None, test_frac=None, use_random_split=None):\n",
    "    train_frac = train_frac if train_frac is not None else TRAIN_FRAC\n",
    "    val_frac = val_frac if val_frac is not None else VAL_FRAC\n",
    "    test_frac = test_frac if test_frac is not None else TEST_FRAC\n",
    "    use_random = use_random_split if use_random_split is not None else USE_RANDOM_SPLIT\n",
    "    assert abs(train_frac + val_frac + test_frac - 1.0) < 1e-9\n",
    "    feat_cols = [c for c in get_feature_columns(df) if c in df.columns]\n",
    "    if not feat_cols:\n",
    "        raise ValueError(\"No feature columns found in df.\")\n",
    "    X = df[feat_cols].astype(float)\n",
    "    y = df[TARGET_COL]\n",
    "    n = len(df)\n",
    "    if use_random:\n",
    "        train_ix, val_ix, test_ix = _random_split_indices(n, train_frac, val_frac, test_frac)\n",
    "    else:\n",
    "        train_ix, val_ix, test_ix = _time_aware_split_indices(n, train_frac, val_frac, test_frac)\n",
    "    X_train = X.iloc[train_ix].copy()\n",
    "    X_val = X.iloc[val_ix].copy()\n",
    "    X_test = X.iloc[test_ix].copy()\n",
    "    y_train = y.iloc[train_ix]\n",
    "    y_val = y.iloc[val_ix]\n",
    "    y_test = y.iloc[test_ix]\n",
    "    scaler = RobustScaler() if SCALER_TYPE == \"robust\" else StandardScaler()\n",
    "    X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=feat_cols, index=X_train.index)\n",
    "    X_val = pd.DataFrame(scaler.transform(X_val), columns=feat_cols, index=X_val.index)\n",
    "    X_test = pd.DataFrame(scaler.transform(X_test), columns=feat_cols, index=X_test.index)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, scaler\n",
    "\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    pred = model.predict(X)\n",
    "    return {\n",
    "        \"mae\": mean_absolute_error(y, pred),\n",
    "        \"rmse\": np.sqrt(mean_squared_error(y, pred)),\n",
    "        \"r2\": r2_score(y, pred),\n",
    "    }\n",
    "\n",
    "\n",
    "def dummy_regression_model():\n",
    "    return DummyRegressor(strategy=\"mean\")\n",
    "\n",
    "\n",
    "def ridge_tuned_model(cv=5):\n",
    "    return RidgeCV(alphas=np.logspace(-3, 2, 100), cv=cv)\n",
    "\n",
    "\n",
    "class RidgeLogTarget:\n",
    "    def __init__(self):\n",
    "        self._model = RidgeCV(alphas=np.logspace(-3, 2, 100), cv=5)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self._model.fit(X, np.log1p(y))\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.expm1(self._model.predict(X))\n",
    "\n",
    "\n",
    "def elastic_net_cv_model(cv=5):\n",
    "    return ElasticNetCV(cv=cv, l1_ratio=[0.1, 0.5, 0.9, 1.0], alphas=np.logspace(-3, 1, 50), max_iter=5000)\n",
    "\n",
    "\n",
    "class RidgeReduced:\n",
    "    \"\"\"Ridge on the same feature set as all models.\"\"\"\n",
    "    def __init__(self):\n",
    "        self._model = RidgeCV(alphas=np.logspace(-2, 2, 50), cv=5)\n",
    "        self._cols = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self._cols = X.columns.tolist()\n",
    "        self._model.fit(X[self._cols], y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self._model.predict(X[self._cols])\n",
    "\n",
    "\n",
    "def random_forest_regression_model(random_state=42):\n",
    "    return RandomForestRegressor(n_estimators=100, max_depth=10, random_state=random_state)\n",
    "\n",
    "\n",
    "def gradient_boosting_model(random_state=42):\n",
    "    return GradientBoostingRegressor(\n",
    "        n_estimators=200, max_depth=4, learning_rate=0.05,\n",
    "        min_samples_leaf=10, subsample=0.8, random_state=random_state,\n",
    "    )\n",
    "\n",
    "\n",
    "def fnn_model(random_state=42):\n",
    "    return MLPRegressor(hidden_layer_sizes=(64, 32), activation=\"relu\", solver=\"adam\", max_iter=1000, random_state=random_state)\n",
    "\n",
    "\n",
    "def svm_model():\n",
    "    return SVR(kernel=\"rbf\", C=1.0, epsilon=0.1)\n",
    "\n",
    "\n",
    "def xgboost_model(random_state=42):\n",
    "    return xgb.XGBRegressor(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=random_state)\n",
    "\n",
    "\n",
    "def hist_gradient_boosting_model(random_state=42):\n",
    "    return HistGradientBoostingRegressor(max_iter=150, max_depth=6, learning_rate=0.05, min_samples_leaf=15, random_state=random_state)\n",
    "\n",
    "\n",
    "def extra_trees_model(random_state=42):\n",
    "    return ExtraTreesRegressor(n_estimators=150, max_depth=12, min_samples_leaf=5, random_state=random_state)\n",
    "\n",
    "\n",
    "def _build_keras_model(n_features, params, seed=42):\n",
    "    keras.backend.clear_session()\n",
    "    tf.random.set_seed(seed)\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.Input(shape=(n_features,)))\n",
    "    units = int(params.get(\"units\", 64))\n",
    "    n_layers = int(params.get(\"n_layers\", 2))\n",
    "    dropout = float(params.get(\"dropout\", 0.2))\n",
    "    l2 = float(params.get(\"l2\", 1e-4))\n",
    "    reg = keras.regularizers.L2(l2) if l2 > 0 else None\n",
    "    for _ in range(n_layers):\n",
    "        model.add(layers.Dense(units, activation=\"relu\", kernel_initializer=\"he_normal\", kernel_regularizer=reg))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dropout(dropout))\n",
    "    model.add(layers.Dense(1, kernel_regularizer=reg))\n",
    "    lr = float(params.get(\"lr\", 1e-3))\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr), loss=\"mse\", metrics=[\"mae\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "class KerasRegressorWrapper:\n",
    "    def __init__(self, params=None, epochs=200, batch_size=32, patience=15, random_state=42):\n",
    "        self.params = params or {\"units\": 64, \"n_layers\": 2, \"dropout\": 0.2, \"lr\": 1e-3}\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.patience = patience\n",
    "        self.random_state = random_state\n",
    "        self.model_ = None\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        X = np.asarray(X, dtype=np.float32)\n",
    "        y = np.asarray(y, dtype=np.float32).reshape(-1, 1)\n",
    "        if X_val is not None:\n",
    "            X_val = np.asarray(X_val, dtype=np.float32)\n",
    "            y_val = np.asarray(y_val, dtype=np.float32).reshape(-1, 1)\n",
    "        self.model_ = _build_keras_model(X.shape[1], self.params, self.random_state)\n",
    "        early = keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\" if X_val is not None else \"loss\",\n",
    "            patience=self.patience,\n",
    "            restore_best_weights=True,\n",
    "        )\n",
    "        fit_kw = {\"epochs\": self.epochs, \"batch_size\": self.batch_size, \"verbose\": 0}\n",
    "        if X_val is not None:\n",
    "            fit_kw[\"validation_data\"] = (X_val, y_val)\n",
    "            fit_kw[\"callbacks\"] = [early]\n",
    "        self.model_.fit(X, y, **fit_kw)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.asarray(X, dtype=np.float32)\n",
    "        return self.model_.predict(X, verbose=0).ravel()\n",
    "\n",
    "\n",
    "def run_all_models(X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "    \"\"\"Train each model; return results list.\"\"\"\n",
    "    models = [\n",
    "        (\"Dummy (mean)\", dummy_regression_model()),\n",
    "        (\"Ridge (CV)\", ridge_tuned_model()),\n",
    "        (\"Ridge (log y)\", RidgeLogTarget()),\n",
    "        (\"ElasticNet (CV)\", elastic_net_cv_model()),\n",
    "        (\"Ridge (reduced)\", RidgeReduced()),\n",
    "        (\"Random Forest\", random_forest_regression_model()),\n",
    "        (\"Gradient Boosting\", gradient_boosting_model()),\n",
    "        (\"Hist Gradient Boosting\", hist_gradient_boosting_model()),\n",
    "        (\"Extra Trees\", extra_trees_model()),\n",
    "        (\"FNN (MLP)\", fnn_model()),\n",
    "        (\"SVR\", svm_model()),\n",
    "        (\"XGBoost\", xgboost_model()),\n",
    "        (\"Keras\", KerasRegressorWrapper(epochs=200, patience=15)),\n",
    "    ]\n",
    "    results = []\n",
    "    for name, model in models:\n",
    "        if \"Keras\" in name:\n",
    "            model.fit(X_train, y_train, X_val=X_val, y_val=y_val)\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "        results.append((name, evaluate_model(model, X_val, y_val), evaluate_model(model, X_test, y_test)))\n",
    "    return results\n",
    "\n",
    "\n",
    "def main():\n",
    "    df = load_data()\n",
    "    n_load = len(df)\n",
    "    df = preprocess_data(df, impute_features=USE_IMPUTATION)\n",
    "    n_pre = len(df)\n",
    "    df = feature_engineering(df)\n",
    "    n_fe = len(df)\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test, _ = training_data_split(df)\n",
    "\n",
    "    print(\"Data: load → preprocess → feature_eng\")\n",
    "    print(f\"  Rows: {n_load} → {n_pre} (impute={USE_IMPUTATION}) → {n_fe}\")\n",
    "    print(f\"  Features: {X_train.shape[1]} cols\")\n",
    "    print(f\"  Split: {'random' if USE_RANDOM_SPLIT else 'time-aware (chronological)'}\")\n",
    "    print(f\"  Train / Val / Test: {len(y_train)} / {len(y_val)} / {len(y_test)}\\n\")\n",
    "\n",
    "    results = run_all_models(X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "\n",
    "    print(\"Model comparison (Validation | Test)\")\n",
    "    print(\"-\" * 72)\n",
    "    print(f\"{'Model':<18} {'Val MAE':>8} {'Val RMSE':>8} {'Val R²':>8}  |  {'Test MAE':>8} {'Test RMSE':>8} {'Test R²':>8}\")\n",
    "    print(\"-\" * 72)\n",
    "    for name, vm, tm in results:\n",
    "        print(f\"{name:<18} {vm['mae']:>8.4f} {vm['rmse']:>8.4f} {vm['r2']:>8.4f}  |  {tm['mae']:>8.4f} {tm['rmse']:>8.4f} {tm['r2']:>8.4f}\")\n",
    "    print(\"-\" * 72)\n",
    "\n",
    "    best_name, _, best_test = max(results, key=lambda r: r[2][\"r2\"])\n",
    "    print(f\"\\nBest by Test R²: {best_name}  (R² = {best_test['r2']:.4f})\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb54d6d-cd7e-426a-82fb-2839d8d58506",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (research2)",
   "language": "python",
   "name": "research2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
